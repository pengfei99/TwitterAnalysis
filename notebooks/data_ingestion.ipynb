{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 01. Data ingestion \n",
    "In this notebook, we will search some twitter message. Then we will call the twitter developper api to download the tweets that we are intrested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requise: Create twitter developer account and generate api credentials\n",
    "Step 1: Apply for a Twitter Developer Account\n",
    "\n",
    "Go to the Twitter developer site to apply for a developer account. Here, you have to select the Twitter user responsible for this account. It should probably be you or your organization. Hereâ€™s what this page looks like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "import os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download tweets\n",
    "## Step1. Setup the twitter api credential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"changeMe\"\n",
    "consumer_secret = \"changeMe\"\n",
    "access_token = \"changeMe\"\n",
    "access_token_secret = \"changeMe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. Create an instance of the twitter client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "client_auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(client_auth, wait_on_rate_limit=True, retry_count=5, retry_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except Exception as e:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Get tweets that you are intrested in \n",
    "\n",
    "We have created the twitter client, now we are ready to search the tweets messages. Here we use the search_tweets to find all the tweets that contains certain key words of certain language. We don't use date to filter tweets because the search index has a 7-day limit. In other words, no tweets will be found for a date older than one week.\n",
    "For more details about the search_tweets\n",
    "https://docs.tweepy.org/en/stable/api.html?highlight=search%20tweet#tweepy.API.search_tweets\n",
    "\n",
    "If you want to search older tweets, you can use the search_full_archive method.\n",
    "https://docs.tweepy.org/en/stable/api.html?highlight=search%20tweet#tweepy.API.search_full_archive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the search filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the search result by using below key words \n",
    "search_words = \"#insee\"\n",
    "# We can get tweet before certain date\n",
    "# until_date = \"2021-11-24\n",
    "# specify the \n",
    "language=\"fr\"\n",
    "# the max tweet number will be retained in the result\n",
    "max_tweet_count=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweets\n",
    "tweets = api.search_tweets(q=search_words,lang=\"fr\",result_type=\"mixed\", count=max_tweet_count)\n",
    "\n",
    "# tweets is a list of object status, which has an attribute _json which is the actual tweet in json string.\n",
    "print(tweets[0]._json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, if we want to get the name of the sender, date and text of a tweet\n",
    "\n",
    "tweet_dict=tweets[0]._json\n",
    "text=tweet_dict.get(\"text\")\n",
    "user_name=tweet_dict.get(\"user\").get(\"name\")\n",
    "date=tweet_dict.get(\"created_at\")\n",
    "print(f\"name:{user_name} | message: {text} | date:{date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get tweets by using twitter api V2. Note to access v2, you need to upgrade your account to a developer or academic account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to use client instead of api\n",
    "tw_client=tw.Client(consumer_key=consumer_key, consumer_secret=consumer_secret, access_token=access_token, access_token_secret=access_token_secret)\n",
    "tweets=tw_client.search_recent_tweets(query=search_words)\n",
    "println(len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4. Generate a data frame \n",
    "\n",
    "Now we have the tweet, we want to generate a dataframe based on the tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tweet_df(tweets):\n",
    "     # init dataframe\n",
    "    df = pd.DataFrame(columns=['name','date','text'])\n",
    "    index=0\n",
    "    for tweet in tweets:\n",
    "        # get column value for each tweet\n",
    "        tweet_dict=tweet._json\n",
    "        text=tweet_dict.get(\"text\")\n",
    "        user_name=tweet_dict.get(\"user\").get(\"name\")\n",
    "        date=tweet_dict.get(\"created_at\")\n",
    "        # add new row to the dataframe\n",
    "        df.loc[index] = pd.Series({'name':user_name, 'date':date, 'text':text})\n",
    "        index=index+1\n",
    "    return df\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=generate_tweet_df(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also get all the fields of a tweet\n",
    "def tweets_json(tweets):\n",
    "    tweet_json=[]\n",
    "    for tweet in tweets:\n",
    "        tweet_json.append(tweet._json)\n",
    "    return tweet_json\n",
    "    \n",
    "\n",
    "# Here we use json normalize to convert json file to a pandas dataframe\n",
    "df_all_atts = pd.json_normalize(tweets_json(tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Write data frame to s3\n",
    "\n",
    "We have the data frame, now we want to save the data frame on s3. We want to save the data frame in format parquet. Because it has an integrated schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Configure s3 connection\n",
    "\n",
    "Here we will set the s3 credential and the output path of the parquet file. As we will generate a parquet file each day. We would like to have the generation date inside the file name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.environ['AWS_S3_ENDPOINT']\n",
    "bucket = \"pengfei\"\n",
    "current_date=date.today().strftime(\"%d-%m-%Y\")\n",
    "output_path = f\"diffusion/demo_prod/tweet_{current_date}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Write df to s3 as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function write a pandas dataframe to s3 in parquet format\n",
    "def write_df_to_s3(df, endpoint, bucket_name, path):\n",
    "    # Convert pandas df to Arrow table\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, filesystem=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df_to_s3(df,endpoint,bucket,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the output parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function read a parquet file and return a arrow table\n",
    "def read_parquet_from_s3(endpoint: str, bucket_name, path):\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    str_info = fs.info(file_uri)\n",
    "    print(f\"input file metadata: {str_info}\")\n",
    "    dataset = pq.ParquetDataset(file_uri, filesystem=fs)\n",
    "    table = dataset.read()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_table=read_parquet_from_s3(endpoint,bucket,output_path)\n",
    "\n",
    "# Convert back to pandas\n",
    "df_new = arrow_table.to_pandas()\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Get old tweets from archive\n",
    "\n",
    "In this section, we will use **API.search_full_archive(label, query, *, tag, fromDate, toDate, maxResults, next)** to collect old tweets.\n",
    "To call this endpoint, you need to have extended rights on your twitter developper account\n",
    "\n",
    "Examples\n",
    "\n",
    "```python\n",
    "# fromDate' must be in format 'yyyyMMddHHmm'\n",
    "from_date=\"202012010000\"\n",
    "end_date=\"202012050000\"\n",
    "\n",
    "tweets = api.search_full_archive(label=\"dev\",query=search_words,fromDate=from_date,toDate=end_date, maxResults=100)\n",
    "print(len(tweets))\n",
    "```\n",
    "\n",
    "Belows are some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# by using the given start_year and end_year, it will generate 5 days date interval for the given year range\n",
    "def generate_dates(start_year,end_year):\n",
    "    dates=[]\n",
    "    months=[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"]\n",
    "    days=[\"01\",\"06\",\"11\",\"16\",\"21\",\"26\"]\n",
    "    for year in range(start_year,end_year):\n",
    "        for month in months:\n",
    "            for day in days:\n",
    "                date=f\"{year}{month}{day}0000\"\n",
    "                dates.append(date)\n",
    "    return dates\n",
    "  \n",
    "# Convert a list of tweet status to a list of tweet message in json\n",
    "def tweets_json(tweets):\n",
    "    tweet_json=[]\n",
    "    for tweet in tweets:\n",
    "        tweet_json.append(tweet._json)\n",
    "    return tweet_json\n",
    "\n",
    "# This function write a pandas dataframe to s3 in parquet format\n",
    "def write_df_to_s3(df, endpoint, bucket_name, path):\n",
    "    # Convert pandas df to Arrow table\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    url = f\"https://{endpoint}\"\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': url})\n",
    "    file_uri = f\"{bucket_name}/{path}\"\n",
    "    pq.write_to_dataset(table, root_path=file_uri, filesystem=fs)\n",
    "    \n",
    "\n",
    "# This function use a start_year and an end_year to get all tweets inside this range, then save to s3   \n",
    "def save_tweets(search_words,start_year,end_year,endpoint,bucket_name,path):\n",
    "    from_date=None\n",
    "    end_date=None\n",
    "    dates=generate_dates(start_year,end_year)\n",
    "    for i in range(0,len(dates),2):\n",
    "        from_date,end_date=dates[i],dates[i+1]\n",
    "        tweets = api.search_full_archive(label=\"dev\",query=search_words,fromDate=from_date,toDate=end_date, maxResults=100)\n",
    "        pdf_tweets = pd.json_normalize(tweets_json(tweets))\n",
    "        if pdf_tweets.empty==False:\n",
    "            print(f\"save {len(pdf_tweets)} tweets\")\n",
    "            write_df_to_s3(pdf_tweets,endpoint,bucket_name,path)\n",
    "        # after each iteration, sleep 60 secs to avoid twitter rate limit 300 request/15mins\n",
    "        time.sleep(60)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Configure save old tweets parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year=2011\n",
    "end_year=2021\n",
    "endpoint = os.environ['AWS_S3_ENDPOINT']\n",
    "bucket = \"pengfei\"\n",
    "output_path=f\"diffusion/demo_prod/old/{start_year}_{end_year}\"\n",
    "search_words = \"insee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequests",
     "evalue": "429 Too Many Requests\nRequest exceeds accountâ€™s current package request limits. Please upgrade your package and retry or contact Twitter about enterprise access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5964/3775112654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_year\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_year\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5964/3366006864.py\u001b[0m in \u001b[0;36msave_tweets\u001b[0;34m(search_words, start_year, end_year, endpoint, bucket_name, path)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mfrom_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_full_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfromDate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoDate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxResults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mpdf_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpdf_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagination_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36msearch_full_archive\u001b[0;34m(self, label, query, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdeveloper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpremium\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpremium\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m    448\u001b[0m             'GET', f'tweets/search/fullarchive/{label}', endpoint_parameters=(\n\u001b[1;32m    449\u001b[0m                 \u001b[0;34m'query'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fromDate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toDate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'maxResults'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'next'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTooManyRequests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTwitterServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTooManyRequests\u001b[0m: 429 Too Many Requests\nRequest exceeds accountâ€™s current package request limits. Please upgrade your package and retry or contact Twitter about enterprise access."
     ]
    }
   ],
   "source": [
    "save_tweets(search_words,start_year,end_year,endpoint,bucket,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
